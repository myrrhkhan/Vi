{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Uses TensorFlow and Keras to train a model from a dataset, exports model. Intended to be run once, and model is intended to be exported.\n",
    "Source: keras documentation, https://keras.io/examples/vision/handwriting_recognition/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace IAM_Words/words.tgz? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "mkdir: data: File exists\n",
      "mkdir: data/words: File exists\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://github.com/sayakpaul/Handwriting-Recognizer-in-Keras/releases/download/v1.0.0/IAM_Words.zip\n",
    "!unzip -qq IAM_Words.zip\n",
    "!\n",
    "!mkdir data\n",
    "!mkdir data/words\n",
    "!tar -xf IAM_Words/words.tgz -C data/words\n",
    "!mv IAM_Words/words.txt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + Seed\n",
    "### Imports:\n",
    "- StringLookup from keras. Part of preprocessing layer, encodes/tokenizes characters\n",
    "- matplotlib. Graphing\n",
    "- tensorflow. ML, includes keras\n",
    "- numpy. Number processing stuff.\n",
    "- os. Operating system stuff.\n",
    "\n",
    "### Note about seed\n",
    "Numpy has its own specific algorithm for its random number generator. That algorithm has an input, and if the exact same input is given, the exact same set of random numbers will appear in the output. (source: https://keras.io/examples/vision/handwriting_recognition/)\n",
    "The documentation includes two lines that set the seed value, which allows anyone following the documentation to get the same results as shown in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/myrrh/Documents/source/Vi/src-py/train-model.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/myrrh/Documents/source/Vi/src-py/train-model.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m StringLookup\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myrrh/Documents/source/Vi/src-py/train-model.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myrrh/Documents/source/Vi/src-py/train-model.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# see note above\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!head -20 data/words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather images\n",
    "### Info about parsing\n",
    "As seen above, the file words.txt has a list of image names and the matching output. Some of the files have \"err\" as a status instead of \"ok,\" so we're skipping those words.\n",
    "In the cell below, we read through words.txt, add the entries to a list (known as words_list), and then finding the filenames and training the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# get list of paths\n",
    "\n",
    "base_path = \"data\" # folder where data is stored\n",
    "words_list = []\n",
    "\n",
    "# open words.txt file, ignore comments, ignore \"err\" labels, add to wordlist\n",
    "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
    "for line in words:\n",
    "    if line[0] == \"#\":\n",
    "        continue\n",
    "    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n",
    "        words_list.append(line)\n",
    "\n",
    "len(words_list)\n",
    "\n",
    "np.random.shuffle(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation, and testing. \n",
    "90:5:5 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "split_idx = int(0.9 * len(words_list))\n",
    "train_samples = words_list[:split_idx]\n",
    "test_samples = words_list[split_idx:]\n",
    "\n",
    "val_split_idx = int(0.5 * len(test_samples))\n",
    "validation_samples = test_samples[:val_split_idx]\n",
    "test_samples = test_samples[val_split_idx:]\n",
    "\n",
    "assert len(words_list) == len(train_samples) + len(validation_samples) + len(\n",
    "    test_samples\n",
    ")\n",
    "\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "base_image_path = os.path.join(base_path, \"words\")\n",
    "\n",
    "'''\n",
    "hierarchy:\n",
    "\n",
    "words\n",
    "├── a01\n",
    "|   ├── a01-000u\n",
    "|   |   ├── a01-000u-00-00.png\n",
    "\n",
    "get_image_paths_and_labels parses through the labels and converts them into paths\n",
    "'''\n",
    "\n",
    "\n",
    "def get_image_paths_and_labels(samples):\n",
    "    \"\"\"\n",
    "    Keyword argument:\n",
    "    samples--list of lines from words.txt. \n",
    "        Represents either training, val, or test data\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    corrected_samples = []\n",
    "    for (i, file_line) in enumerate(samples):\n",
    "        line_split = file_line.strip()      # remove spaces from ends\n",
    "        line_split = line_split.split(\" \")  # split by space\n",
    "\n",
    "        # Each line split will have this format for the corresponding image:\n",
    "        # part1/part1-part2/part1-part2-part3.png\n",
    "        image_name = line_split[0]          # ex: a01-000u-00-00\n",
    "        partI = image_name.split(\"-\")[0]    # ex: a01, first folder\n",
    "        partII = image_name.split(\"-\")[1]   # ex: 000u, ending of second folder\n",
    "        img_path = os.path.join(\n",
    "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
    "        ) \n",
    "        # \"data/words\" + \"a01\" + \"ao1-000u\" + \"a01-000u-00-00.png\"\n",
    "        if os.path.getsize(img_path):\n",
    "            paths.append(img_path) # \"data/words/a01/ao1-000u/a01-000u-00-00.png\"\n",
    "            # original, ex \"a01-000u-00-00 ok 154 408 768 27 51 AT A\"\n",
    "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
    "\n",
    "    return paths, corrected_samples\n",
    "\n",
    "\n",
    "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
    "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
    "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Find maximum length and the size of the vocabulary in the training data.\n",
    "train_labels_cleaned = []\n",
    "characters = set()                          # O(1) time, all chars in training data\n",
    "max_len = 0\n",
    "\n",
    "for label in train_labels:\n",
    "    label = label.split(\" \")[-1].strip()    # label for image\n",
    "    for char in label:\n",
    "        characters.add(char)\n",
    "\n",
    "    max_len = max(max_len, len(label))      # continuously find max length\n",
    "    train_labels_cleaned.append(label)      # add word label to list\n",
    "\n",
    "characters = sorted(list(characters))\n",
    "with open(\"./models/characters.txt\", \"w\") as charfile:\n",
    "    for char in characters:\n",
    "        charfile.write(\" \".join(char) + \" \")\n",
    "\n",
    "print(\"Maximum length: \", max_len)\n",
    "print(\"Vocab size: \", len(characters))\n",
    "\n",
    "# Check some label samples.\n",
    "train_labels_cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and test data: clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def clean_labels(labels):\n",
    "    cleaned_labels = []\n",
    "    for label in labels:\n",
    "        label = label.split(\" \")[-1].strip()\n",
    "        cleaned_labels.append(label)\n",
    "    return cleaned_labels\n",
    "\n",
    "\n",
    "validation_labels_cleaned = clean_labels(validation_labels)\n",
    "test_labels_cleaned = clean_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building character vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Mapping characters to integers.\n",
    "char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n",
    "\n",
    "# Mapping integers back to original characters.\n",
    "num_to_char = StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "padding_token = 99\n",
    "image_width = 128\n",
    "image_height = 32\n",
    "\n",
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize_with_pad(image, h, w)\n",
    "    # documentation had a different thing with \n",
    "    # tf.image.resize, find padding diff, then tf.pad\n",
    "    # found this function instead and I guess it works fine\n",
    "\n",
    "    # lines below convert from vertical to horizontal, then flips to correct orientation\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image_path, img_size=(image_width, image_height)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, 1) # 1: output to grayscale. tensor of uint8 or uint16\n",
    "    image = distortion_free_resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # cast to float instead of int\n",
    "    return image\n",
    "\n",
    "def vectorize_label(label):\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\")) # call StringLookup layer\n",
    "    # resize label layer to fit max size\n",
    "    length = tf.shape(label)[0]\n",
    "    pad_amount = max_len - length \n",
    "    label = tf.pad(label, paddings=[[0, pad_amount]], constant_values=padding_token)\n",
    "    return label\n",
    "\n",
    "def process_image_labels(image_path, label):\n",
    "    # calls above functions, gets preprocessed image and label, returns as dict\n",
    "    image = preprocess_image(image_path)\n",
    "    label = vectorize_label(label)\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "def prepare_dataset(image_paths, labels):\n",
    "    # calls all functions above, makes tf dataset with image paths, \n",
    "    # maps image paths and labels to tf images and tf labels\n",
    "    # TODO look up AUTOTUNE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(\n",
    "        process_image_labels, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train_ds = prepare_dataset(\n",
    "    train_img_paths, \n",
    "    train_labels_cleaned\n",
    ")\n",
    "validation_ds = prepare_dataset(\n",
    "    validation_img_paths, \n",
    "    validation_labels_cleaned\n",
    ")\n",
    "test_ds = prepare_dataset(\n",
    "    test_img_paths, \n",
    "    test_labels_cleaned\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "for data in train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 8))\n",
    "\n",
    "    for i in range(16):\n",
    "        img = images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8) # convert to np array\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        label = labels[i]\n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n",
    "        # convert to string\n",
    "        label = tf.strings.reduce_join(num_to_char(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model, using Connectionist Temporal Classification (CTC) loss\n",
    "Keras documentation linked this artcle describing CTC loss: https://distill.pub/2017/ctc/\n",
    "My notes:\n",
    "- used when input X and input Y don't have consistent lengths or ratios or correspondence between each other. \n",
    "    - Ex: audio -> transcript, audio length is diff from transcript length, we don't know which audio portion corresponds to which transcript portion\n",
    "- find probability that Y matches with X, gradient descent to minimize loss\n",
    "- creates blank token between distinct characters, ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "@tf.keras.saving.register_keras_serializable(package=\"CTCLayer\")\n",
    "class CTCLayer(tf.keras.layers.Layer):\n",
    "    def __init__ (self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost # set loss func to ctc\n",
    "\n",
    "    # tensor-in tensor-out computation function\n",
    "    # from https://keras.io/api/layers/\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        # find loss between prediction and true\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    # inputs: convert images and labels as keras inputs\n",
    "    input_img = tf.keras.Input(shape=(image_width, image_height, 1), name=\"image\")\n",
    "    labels = tf.keras.layers.Input(name=\"label\", shape=(None,))\n",
    "\n",
    "    # first conv block\n",
    "    # takes a bunch of filters and finds which filters have found notable features\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        32, # 32 random filters, find best filters\n",
    "        (3, 3), # 3x3 filter\n",
    "        activation=\"relu\", # remove negative number\n",
    "        # initialize weights, prevents vanishing gradients\n",
    "        # sets initial weights to be larger than normal, \n",
    "        # gaussian dist mean 0 variance 2/n\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\", # even padding\n",
    "        name=\"Conv1\"\n",
    "    )(input_img)\n",
    "    # max pooling: reduce image size by taking highest value in 2x2 matrix\n",
    "    # retain features found in convolution\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # second conv block\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3), # 3x3 filter\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\", # even padding\n",
    "        name=\"Conv2\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # feature maps are 4x smaller, resize before passing to RNN\n",
    "    new_shape = ((image_width // 4), (image_height // 4) * 64)\n",
    "    x = tf.keras.layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    # shallow neural network:\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    # randomly sets inputs to 0 to prevent overfitting, drops 20% of units\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    # recurrent neural networks:\n",
    "    # TODO look up\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.25)\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.25)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        len(char_to_num.get_vocabulary()) + 2, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # add CTC layer\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # define model\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"handwriting_recognizer\"\n",
    "    )\n",
    "    opt = tf.keras.optimizers.legacy.Adam() # optimizers.Adam() runs slowly on M1/M2 macs\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "validation_images = []\n",
    "validation_labels = []\n",
    "\n",
    "for batch in validation_ds:\n",
    "    validation_images.append(batch[\"image\"])\n",
    "    validation_labels.append(batch[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "epochs = 50  # To get good results this should be at least 50.\n",
    "dotkeras_path = f\"./models/{epochs}_epochs.keras\"\n",
    "checkpoint_path = f\"./checkpoints/{epochs}_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def calculate_edit_distance(labels, predictions):\n",
    "    # Get a single batch and convert its labels to sparse tensors.\n",
    "    saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n",
    "\n",
    "    # Make predictions and convert them to sparse tensors.\n",
    "    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    predictions_decoded = tf.keras.backend.ctc_decode(\n",
    "        predictions, input_length=input_len, greedy=True\n",
    "    )[0][0][:, :max_len]\n",
    "    sparse_predictions = tf.cast(\n",
    "        tf.sparse.from_dense(predictions_decoded), dtype=tf.int64\n",
    "    )\n",
    "\n",
    "    # Compute individual edit distances and average them out.\n",
    "    edit_distances = tf.edit_distance(\n",
    "        sparse_predictions, saprse_labels, normalize=False\n",
    "    )\n",
    "    return tf.reduce_mean(edit_distances)\n",
    "\n",
    "\n",
    "class EditDistanceCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, pred_model):\n",
    "        super().__init__()\n",
    "        self.prediction_model = pred_model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        edit_distances = []\n",
    "\n",
    "        for i in range(len(validation_images)):\n",
    "            labels = validation_labels[i]\n",
    "            predictions = self.prediction_model.predict(validation_images[i])\n",
    "            edit_distances.append(calculate_edit_distance(labels, predictions).numpy())\n",
    "\n",
    "        print(\n",
    "            f\"Mean edit distance for epoch {epoch + 1}: {np.mean(edit_distances):.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/66138748/tensorflow-2-3-load-model-from-modelcheckpoint-callback-with-both-custom-layers\n",
    "class SaveModelH5(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, pred_model):\n",
    "        super().__init__()\n",
    "        self.prediction_model = pred_model\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "         self.val_loss = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "        self.val_loss.append(logs.get(\"val_loss\"))\n",
    "        if current_val_loss <= min(self.val_loss):\n",
    "            print('Find lowest val_loss. Saving entire model.')\n",
    "            self.prediction_model.save(checkpoint_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "prediction_model = tf.keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "# prediction_model = tf.keras.models.load_model(dotkeras_path)\n",
    "edit_distance_callback = EditDistanceCallback(prediction_model)\n",
    "# checkpoint callback\n",
    "cp_callback = SaveModelH5(prediction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[edit_distance_callback, cp_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!mkdir -p models\n",
    "prediction_model.save(dotkeras_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# A utility function to decode the output of the network.\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_len\n",
    "    ]\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "#  Let's check results on some test samples.\n",
    "for batch in test_ds.take(1):\n",
    "    batch_images = batch[\"image\"]\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 8))\n",
    "\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    for i in range(16):\n",
    "        img = batch_images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure model can be reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pred2 = tf.keras.models.load_model(dotkeras_path)\n",
    "\n",
    "# test pred2\n",
    "for batch in test_ds.take(1):\n",
    "    batch_images = batch[\"image\"]\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 8))\n",
    "    print(f\"Image shape: f{tf.shape(batch_images[0])}\")\n",
    "\n",
    "    preds = pred2.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    for i in range(16):\n",
    "        img = batch_images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check individual image via path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def predict(img_path: str, dotkeras_path: str) -> str:\n",
    "    dataset = prepare_dataset([img_path], [\"null\"])\n",
    "    for batch in dataset:\n",
    "        batch_image = batch[\"image\"]\n",
    "        pred = prediction_model.predict(batch_image)\n",
    "        pred_text = decode_batch_predictions(pred)\n",
    "\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "predict('./data/words/a02/a02-000/a02-000-00-04.png', './models/50_epochs.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check entire testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/myrrh/.pyenv/versions/3.10.13/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/myrrh/.pyenv/versions/3.10.13/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "\n",
    "for batch in test_ds:\n",
    "    batch_images, batch_labels = batch[\"image\"], batch[\"label\"]\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    for i in range(len(batch_images)):\n",
    "        img = batch_images[i]\n",
    "        label = batch_labels[i]\n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n",
    "        # convert to string\n",
    "        label = tf.strings.reduce_join(num_to_char(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "        prediction = pred_texts[i]\n",
    "        if label == prediction:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            num_incorrect += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"{num_correct}/{total} correct, {(num_correct/total) * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
